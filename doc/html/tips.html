<!DOCTYPE html>
<html>
  <head>
    <title>SubMIT - Tips and guidelines</title>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <link href="/css/common.css" rel="stylesheet">
  </head>
  <body>
    Below are some tips and guidelines for submitting condor jobs from subMIT.

    <h3>Index</h3>
    <ul>
      <li><a href="#general">General notes</a></li>
      <li>
        <a href="#requirements">The requirements line</a>
        <ul>
          <li><a href="#manual">Examples for writing the requirements by hand</a></li>
        </ul>
      </li>
      <li>
        <a href="#tier2">Migrating from CMS MIT T2</a>
        <ul>
          <li><a href="#restrict-tier2">Restricting your jobs to MIT T2</a></li>
          <li><a href="#tier2-difference">Important differences between SubMIT and T2</a></li>
          <li><a href="#deprecated">Operations no longer possibled at SubMIT</a></li>
          <li><a href="#t2storage">Copy tools and file permissions</a></li>
          <li><a href="#cmssw">Running a CMSSW job</a></li>
        </ul>
      </li>
      <li>
        <a href="#tricks">Condor tricks</a>
        <ul>
          <li><a href="jobid">Making jobs identifiable</a></li>
          <li><a href="logchirp">Using condor_chirp for semi-realtime logging</a></li>
        </ul>
      </li>
    </ul>

    <hr>

    <section id="general">
      <h1>General notes</h1>
      <ul>
        <li>Probability of failure of individual jobs is always nonzero in a grid environment. Your task should consist of dispensable jobs, and not rely on every job returning successfully.</li>
        <li>There is a scalability limit to HTCondor. Total number of jobs in the queue cannot exceed 180,000. User jobs may be removed from the queue when necessary.</li>
      </ul>
    </section>

    <section id="requirements">
      <h1>The requirements line</h1>
      <p>The "requirements" line of the job description can be used to restrict the execute nodes to be used. There is a script to generate the requirements string. Use
<span class="codeblock">
  reqgen.py {user}
</span>
        to print out a user-customized requirements string that can be pasted into a condor job description. There are many options to the script to fine-tune the execution targets. See the help menu (-h option) of the script for details.</p>
      
      <article id="manual">
        <h2>Examples for writing the requirements by hand</h2>
        See <a href="clusters.html">the notes on individual clusters</a> for a full list of configurations to use for various clusters.

        <ul>
          <li>Example 1: Run only on MIT Campus Factory (CMS HEP user)
<span class="codeblock">
requirements = GLIDEIN_Site == "MIT_CampusFactory" && (BOSCOGroup == "paus" || BOSCOGroup == "bosco_cms") && HAS_CVMFS_cms_cern_ch
</span>
            For other users, "bosco_cms" should be replaced by "bosco_cmshi" (CMS HI) or "bosco_lns" (other LNS). HAS_CVMFS_cms_cern_ch is only necessary if you need CMS CVMFS.
          </li>
          <li>Example 2: Running on a specific MIT resource, e.g. T2_US_MIT
<span class="codeblock">
requirements = GLIDEIN_Site == "MIT_CampusFactory" && BOSCOCluster == "ce03.cmsaf.mit.edu" && (BOSCOGroup == "paus" || BOSCOGroup == "bosco_cms") && HAS_CVMFS_cms_cern_ch
</span>
          </li>
          <li>Example 3: Enabling OSG
<span class="codeblock">
+ProjectName = MyProject
requirements = \
  (OSGVO_OS_STRING == "RHEL 6" && HAS_CVMFS_cms_cern_ch) || \
  (GLIDEIN_Site == "MIT_CampusFactory" && (BOSCOGroup == "paus" || BOSCOGroup == "bosco_cms") && HAS_CVMFS_cms_cern_ch)
</span>
          </li>
          <li>Example 4: Adding US CMS
<span class="codeblock">
+ProjectName = MyProject
+DESIRED_Sites = "site1,site2,..."
requirements = \
  (OSGVO_OS_STRING == "RHEL 6" && HAS_CVMFS_cms_cern_ch) || \
  GLIDEIN_REQUIRED_OS == "rhel6" || \
  (GLIDEIN_Site == "MIT_CampusFactory" && (BOSCOGroup == "paus" || BOSCOGroup == "bosco_cms") && HAS_CVMFS_cms_cern_ch)
</span>
          </li>
          <li>Example 5: Avoiding certain sites
<span class="codeblock">
+ProjectName = MyProject
+DESIRED_Sites = "site1,site2,..."
requirements = ( \
  (OSGVO_OS_STRING == "RHEL 6" && HAS_CVMFS_cms_cern_ch) || \
  GLIDEIN_REQUIRED_OS == "rhel6" || \
  (GLIDEIN_Site == "MIT_CampusFactory" && (BOSCOGroup == "paus" || BOSCOGroup == "bosco_cms") && HAS_CVMFS_cms_cern_ch) \
) && ( \
  isUndefined(GLIDEIN_Entry_Name) || \
  !stringListMember(GLIDEIN_Entry_Name, "CMS_T2_US_Nebraska_Red_op,CMS_T2_US_Nebraska_Red_gw1_op,CMS_T2_US_Nebraska_Red_gw2_op,CMS_T3_MX_Cinvestav_proton_work,CMS_T3_US_Omaha_tusker,CMSHTPC_T3_US_Omaha_tusker,Glow_US_Syracuse_condor,Glow_US_Syracuse_condor-ce01,Gluex_US_NUMEP_grid1,HCC_US_BNL_gk01,HCC_US_BNL_gk02,HCC_US_BU_atlas-net2,OSG_US_FIU_HPCOSGCE,OSG_US_Hyak_osg,OSG_US_UConn_gluskap,OSG_US_SMU_mfosgce", ",") \
)                               
</span>
          </li>
        </ul>
      </article>
    </section>

    <section id="tier2">
      <h1>Migrating from CMS MIT T2</h1>

      <article id="restrict-tier2">
        <h2>Restricting your jobs to MIT T2</h2>
        <p>While you have access to a larger computing pool than MIT T2, some of your jobs may be optimized or designed to be run at MIT T2. To restrict your jobs to only run at MIT, add the following to your requirements.
          <span class="codeblock">
  requirements = GLIDEIN_Site == "MIT_CampusFactory" && BOSCOCluster == "ce03.cmsaf.mit.edu" && BOSCOGroup == "@group@" && HAS_CVMFS_cms_cern_ch
          </span>
          Where <span class="code">@group@</span> is <span class="code">bosco_cms</span> for MIT CMS HEP collaborators, <span class="code">bosco_cmshi</span> for MIT CMS HIG collaborators, and <span class="code">bosco_lns</span> for other LNS members.</p>
      </article>

      <article id="tier2-difference">
        <h2>Important differences between SubMIT and T2</h2>
        <ul>
          <li>SubMIT does not have a large user local space. Large job output should be copied to T2 /mnt/hadoop via gfal-copy or lcg-cp in the job executable. If you restrict your jobs to run at T2, it is possible to use normal cp and mv, but the commands will be executed by the user of the BOSCOGroup.</li>
          <li>HTCondor at SubMIT cannot access the user home directory (AFS authenticates with kerberos, the system user condor does not have your ticket). All job inputs and scripts should be placed within /work/{user}.</li>
        </ul>
      </article>

      <article id="deprecated">
        <h2>Operations no longer possible at SubMIT</h2>
        <ul>
          <li>Log in to a worker node to read the output of a running job. You can instead <a href="#logchirp">use condor_chirp to make the executables send the log file.</a></li>
          <li>Anything T2 local, unless you restrict your jobs to T2 only.</li>
        </ul>
      </article>
      
      <article id="t2storage">
        <h2>Copy tools and file permissions</h2>
        <p>Recommended method for writing to T2 storage is lcg-cp or gfal-copy. By reading inputs via CVMFS or xrootd and writing the output with such grid tools, your jobs will be fully unbound and can run on any resource.</p>
        <p>To copy back the job output to T2, use the following lines at the end of your executable script (bash):
          <span class="codeblock">
  if which gfal-copy
  then
    gfal-copy file://$PWD/output_file srm://se01.cmsaf.mit.edu:8443/srm/v2/server?SFN=destination_full_path
  else
    lcg-cp -v -D srmv2 -b file://$PWD/output_file srm://se01.cmsaf.mit.edu:8443/srm/v2/server?SFN=destination_full_path
  fi
          </span>
          In a highly heterogeneous pool, it is possible that the worker node does not have either of the command installed. In such a case the output is not retrievable; you may want to check the command availability at the very beggining of your script and abort the job immediately if there is no way to get the output back.</p>
        <p>Files written by gfal-copy or lcg-cp will be owned by a "grid user". To delete them, use gfal-rm from SubMIT. This also means that the output directory must have proper permissions. The grid copy tools will create whatever directories that do not exist, as long as allowed by the permission settings.</p>
      </article>

      <article id="cmssw">
        <h2>Running a CMSSW job</h2>
        <ul>
          <li>/cvmfs/cms.cern.ch is mounted on SubMIT, and therefore most of CMSSW setup should work without additional changes.</li>
          <li>The only difference is in Frontier (i.e. GlobalTag) connection. The node is not part of any CMS site and therefore needs to be manually pointed to a squid proxy:
	    <span class="codeblock">
process.GlobalTag.connect = 'frontier://(proxyurl=http://squid.cmsaf.mit.edu:3128)(proxyurl=http://squid1.cmsaf.mit.edu:3128)(proxyurl=http://squid2.cmsaf.mit.edu:3128)(serverurl=http://cmsfrontier.cern.ch:8000/FrontierProd)/CMS_CONDITIONS'
	    </span>
	    This is only necessary when executing cmsRun directly on submit.mit.edu. Be sure to remove the line for the CMSSW configuration that is shipped to the condor pool.
          </li>
        </ul>
      </article>
    </section>

    <section id="tricks">
      <h1>Condor tricks</h1>

      <article id="jobid">
        <h2>Making jobs identifiable</h2>
        <p>A standard technique to make your jobs identifiable is to pass the ClusterId and ProcessId of the job to the job as command-line arguments. This is done by using the arguments line of the job description:
          <span class="codeblock">
  arguments = {other arguments} $(ClusterId) $(ProcessId) {other arguments}
          </span>
        </p>
      </article>
      
      <article id="logchirp">
        <h2>Using condor_chirp for semi-realtime logging</h2>
        <p style="color:red;">Only for test and debugging.</p>
        <p>Condor comes with a command <a href="http://research.cs.wisc.edu/htcondor/manual/current/condor_chirp.html">condor_chirp</a> to allow communication (file read/write and more) between a running job and the submitter. You can use this mechanism to ship the stdout and stderr logs as they are being written by your executable. To do this, add a line in your job description:
          <span class="codeblock">
  +WantIOProxy = true
          </span>
          and add lines like below in your executable script whenever you want the job to report back.
          <span class="codeblock">
  $(condor_config_val LIBEXEC)/condor_chirp put _condor_stdout _condor_stdout.{some_identifier_string}
          </span>
          Replace stdout with stderr for error output. You can pass the ClusterId and ProcessId to the job as discussed above and use them as the identifier. The files are copied to the directory where condor_submit command was issued, or to <span class="code">initialdir</span> if it is specified in the job description.
        </p>
      </article>
    </section>
  </body>
</html>
