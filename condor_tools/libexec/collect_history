#!/usr/bin/env python

"""
colllect_history - Collect job information from schedd and save into database.
"""

import sys
import os
import logging
import pwd
import time

logging.basicConfig(format = '%(asctime)s:%(levelname)s:%(name)s: %(message)s', level = logging.INFO)
LOG = logging.getLogger()

import htcondor

import condor_tools.collect_history as ch
from condor_tools.history_db import HistoryDB

# Increment this number whenever ClusterId counter is reset
ch.CONDOR_INSTANCE = 1

schedd = htcondor.Schedd()
db = HistoryDB()

# Get the set of open cluster IDs
clusters_in_queue = ch.get_clusters_in_queue(schedd)
open_clusters = clusters_in_queue | ch.get_open_clusters(db)

# Get the list of classads for jobs in open_clusters
all_ads = ch.get_history_ads(open_clusters)

LOG.info('Processing %d history ads.' % len(all_ads))

# Some efficiency measures
cluster_jobs = dict()

for jobads in all_ads:
    cluster_id = jobads['ClusterId']
    proc_id = jobads['ProcId']

    LOG.debug('Job %d.%d' % (cluster_id, proc_id))

    if 'LastMatchTime' not in jobads:
        LOG.debug('LastMatchTime not defined. Skipping job.')
        # This job was not matched; we will not record it
        continue

    if cluster_id not in cluster_jobs:
        # First time encountering this cluster in the current cycle
        # Fill cluster_jobs from DB, or if the cluster is entirely new, insert into DB
        cluster_jobs[cluster_id] = ch.get_cluster_jobs(jobads, db)

    if proc_id in cluster_jobs[cluster_id]:
        # We know about this job already
        continue

    ch.insert_one_job(jobads, db)
    cluster_jobs[cluster_id].add(proc_id)

# save currently open clusters
LOG.info('Exiting cycle with %d open clusters.' % len(clusters_in_queue))
LOG.debug('Current open clusters: %s', ' '.join('%d' % c for c in clusters_in_queue))

db.query('TRUNCATE TABLE `open_clusters`')
if len(clusters_in_queue) != 0:
    db.query('INSERT INTO `open_clusters` VALUES %s' % (','.join(['(%d)' % cluster_id for cluster_id in clusters_in_queue])))
